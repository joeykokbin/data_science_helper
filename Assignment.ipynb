{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3caa271-9ec0-4f76-b7cc-2c6e968d8a47",
   "metadata": {},
   "source": [
    "## Intialisation  \n",
    "\n",
    "Loading of packages and files. There are some packages which will be loaded at a later stage for ease of reference, though I understand that the best practice is to import all required packages up front."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c17ff8d-027a-4ea0-839c-47167e6ec2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "# For encoding of hashed id into something easier to tag\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Maps Requires folium package - requires branca and python-abi as dependencies.\n",
    "# Installed using conda-forge. On government workstation may not be so straightforward due to internet separation.\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "# For text mining\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c92ea3fc-cc20-4f55-a1da-4e38e638d553",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"sd_sample_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eb7ce3cf-43f1-40f2-a7b1-39aa6ac2961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90af292-c0e0-42d3-8350-1916f9429829",
   "metadata": {},
   "source": [
    "## PRE-PROCESSING  \n",
    "\n",
    "This section here details the rough steps to process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a7feeebb-a775-41da-890e-d4aa2afa6d49",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data2 = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0bd1303a-5ade-4016-82c6-be1242eeced2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Case Record Key', 'Received Date', 'Received Time',\n",
       "       'Case Owner Agency', 'Mobile Categories', 'Mobile Sub Categories',\n",
       "       'Description', 'Incident Block', 'Incident Street Name',\n",
       "       'Incident Postal Code', 'Channel', 'address', 'location_desc',\n",
       "       'Customer_ID', 'latitude', 'longitude'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "24bdf936-2772-4f73-87a0-b79be74f070d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Columns are renamed for easier wrangling\n",
    "newcols = [\"CASE ID\", \"DATE\", \"TIME\", \"AGENCY\", \"CATEGORY\", \"SUBCATEGORY\", \"DESCRIPTION\", \"BLOCK\", \"STREET_NAME\", \"POSTAL_CODE\", \"CHANNEL\", \"ADDRESS\", \"LOCATION_DESC\", \"CUST_ID\", \"LATITUDE\", \"LONGITUDE\"]\n",
    "data2.columns = newcols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6afbc837-fe00-45b3-b183-bc4267385dc8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HDB' 'NParks' 'Enterprise SG' 'SFA' 'LTA' 'NEA' 'PA']\n"
     ]
    }
   ],
   "source": [
    "# Check unique values\n",
    "print(data2[\"AGENCY\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c276b9a7-f4a0-4a49-9c39-cb7209830a67",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Can be used as variable if needed to convert back\n",
    "agency_dict = {\"NParks\":\"NPARKS\", \"Enterprise SG\":\"ESG\"}\n",
    "\n",
    "# Replace so that it is easier to tag.\n",
    "data2[\"AGENCY\"] = data2[\"AGENCY\"].replace(agency_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e5441d18-7f60-45e8-9379-c0d9ee275af6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Safe Distancing']\n",
      "['OS App']\n"
     ]
    }
   ],
   "source": [
    "# Only safe distancing\n",
    "print(data2[\"CATEGORY\"].unique())\n",
    "\n",
    "# Only OS App. Perhaps andriod version not developed then (May 2020).\n",
    "print(data2[\"CHANNEL\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b78fc71a-a3de-48e1-a468-890674c70059",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For consistency\n",
    "data2[\"CATEGORY\"] = data2[\"CATEGORY\"].apply(str.upper)\n",
    "data2[\"CHANNEL\"] = data2[\"CHANNEL\"].apply(str.upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f1d71c3b-6e6f-4a0b-8719-e513d35ed227",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For purposes of this assessment, we can remove these columns (as they have same values throughout)\n",
    "# However, if we have the full data, we can perform further analysis from it.\n",
    "# The type of analysis required would depend on the problem statement being asked.\n",
    "data2 = data2.drop(columns = [\"CATEGORY\", \"CHANNEL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0de4a15f-e048-4ddb-bde0-efb6ee08fa29",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HDB Town/Neighbourhood Ctrs & HDB Common Areas' 'Parks & PCNs'\n",
      " 'Malls and Commercial Areas' 'Coffeeshops & F&B in HDB Estates'\n",
      " 'Transport Nodes' 'Hawker Ctrs & Wet Markets' 'Community Clubs']\n"
     ]
    }
   ],
   "source": [
    "# Provide easier keys to categorize later on\n",
    "print(data2[\"SUBCATEGORY\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cf8d247b-2f60-47f2-98bd-9303e68e60f5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "subcat_dict = {\"HDB Town/Neighbourhood Ctrs & HDB Common Areas\":\"HDB_ESTATE_AREA\", \"Parks & PCNs\":\"PARKS_PCNS\", \n",
    "     \"Malls and Commercial Areas\": \"MALLS_COMMERCIALS\", \"Coffeeshops & F&B in HDB Estates\": \"HDB_F&B_COFFEESHOP\", \n",
    "     \"Transport Nodes\": \"TRANSPORT_NODES\", \"Hawker Ctrs & Wet Markets\": \"HAWKER_WETMKT\",\"Community Clubs\": \"COMMUNITY_CLUBS\"}\n",
    "data2[\"SUBCATEGORY\"] = data2[\"SUBCATEGORY\"].replace(subcat_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b2687cec-ce16-4e81-be32-3afae487ca42",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#data2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "72cd0a17-208e-41c0-bb31-b9f8e6f91fd1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine date and time together\n",
    "data2[\"DATETIME\"] = data2[\"DATE\"] + \" \" + data2[\"TIME\"]\n",
    "data2[\"DATETIME\"] = pd.to_datetime(data2[\"DATETIME\"], format = \"%d/%m/%Y %I:%M:%S %p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "14a74406-46e1-4b4f-97d5-f8602bc0157f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove date and time columns as the info is subsumed in datetime\n",
    "data3 = data2.drop(columns = [\"DATE\", \"TIME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "986cbeb8-863e-4a8f-bc38-b87e4bbd5876",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CUST_ID can be used to differentiate customer. To convert it into something easier, like case id.\n",
    "\n",
    "label_enc = preprocessing.LabelEncoder()\n",
    "test = label_enc.fit_transform(data3[\"CUST_ID\"])\n",
    "\n",
    "data3[\"NEWCUST_ID\"] = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4ec27205-0a00-40cb-ad1f-7fcbd9fedfb7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To keep just in case\n",
    "unused_df = data3.loc[:, [\"CASE ID\", \"CUST_ID\", \"NEWCUST_ID\"]]\n",
    "data3 = data3.drop(columns = \"CUST_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fd2c4f07-1afd-4f26-ae25-3e3053dd95e3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#data3.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9ca6fcd6-b345-4620-8f6b-988c53122c60",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only run if you have not downloaded the stopwords\n",
    "# Ensure that the downloaded package is in a folder that exists in nltk.data.path. Otherwise have to add it.\n",
    "# nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba2471a-76b4-4793-ba16-43d095d7e10f",
   "metadata": {},
   "source": [
    "## Analysis  \n",
    "\n",
    "This section will analyse the data and attempt to generate insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b3b7dbe9-c112-4fed-b930-94648506e25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latitude and longitude of Singapore.\n",
    "sg = [1.3521, 103.8198]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43fc2bd-e90b-432c-aabd-1c8ca28b6e6b",
   "metadata": {},
   "source": [
    "__Create functions for ease of visualisation and processing.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51963016-78ac-4bc6-b42a-d3ae4b2fb0ee",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_folium(df, location = [1.3521, 103.8198], tiles = \"CartoDB positron\", zoom_start = 11, filenm = \"output.html\"):\n",
    "    \n",
    "    '''\n",
    "    Function to create folium maps.\n",
    "    \n",
    "    '''\n",
    "    location_list = [[a,b] for a, b in zip(df[\"LATITUDE\"], df[\"LONGITUDE\"])]\n",
    "    \n",
    "    mymap = folium.Map(location = location, tiles = tiles, zoom_start = zoom_start)\n",
    "    \n",
    "    # Add market clusters.\n",
    "    \n",
    "    marker_cluster = MarkerCluster().add_to(mymap)\n",
    "\n",
    "    for point in range(0, len(location_list)):\n",
    "        popup = \"Location:\" + str(df[\"POSTAL_CODE\"].iloc[point]) +  \"/n\" + df[\"DESCRIPTION\"].iloc[point]\n",
    "        folium.Marker(location_list[point], popup = popup).add_to(marker_cluster)\n",
    "\n",
    "    mymap.save(filenm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb10541f-3bb0-4c36-bd4c-5405af970131",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create function for data processing.\n",
    "\n",
    "def text_mine(text, punctuation = \"!@#$%^&*()_+<>?:.,;\", stop_words = None):\n",
    "    \n",
    "    ''' \n",
    "    Will tokenize sentence to words.\n",
    "    Replace punctuations with \"\" i.e. remove them.\n",
    "    Remove stop words.\n",
    "    Return list of words & elements. \n",
    "    \n",
    "    Requires nltk.word_tokenize\n",
    "    '''\n",
    "    if stop_words == None:\n",
    "        stop_words = set(stopwords.words(\"English\"))\n",
    "    \n",
    "    if type(text) == list:\n",
    "        text = \" \".join(text)\n",
    "    \n",
    "    # Remove punctuations\n",
    "    word_text = word_tokenize(text)\n",
    "    new_words = []\n",
    "    for word in word_text:\n",
    "        for character in word:\n",
    "            if character in punctuation:\n",
    "                word = word.replace(character, \"\")\n",
    "                \n",
    "        # Word tokenize will separate punctuations\n",
    "        # Punctuations will be replaced with \"\". No need to append.\n",
    "        if word != \"\":\n",
    "            new_words.append(word)\n",
    "            \n",
    "    # Conduct stemming to group same words (walk and walking)\n",
    "    # Not done for readability in wordcloud output.\n",
    "    #stemmer = PorterStemmer()\n",
    "    #stemmed_words = [stemmer.stem(a) for a in new_words]\n",
    "    stemmed_words = new_words.copy()\n",
    "    \n",
    "    filtered = []\n",
    "    for words in stemmed_words:\n",
    "        if words not in stop_words:\n",
    "            filtered.append(words)\n",
    "    \n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "951544d2-9f06-4744-9141-d45701a323ba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create function for wordcloud\n",
    "def create_save_cloud(text, max_words = 100, bg_color = \"white\", filenm = \"Output.png\"):\n",
    "    ''' Requires installation/imports of wordcloud'''\n",
    "    \n",
    "    if type(text) == list:\n",
    "        input_txt = \" \".join(text)\n",
    "\n",
    "    word_cloud = wordcloud.WordCloud(max_words = max_words, background_color = bg_color, width=1600, height=800).generate(input_txt)\n",
    "\n",
    "    # Display the generated image:\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.imshow(word_cloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    #plt.show()\n",
    "    plt.savefig(filenm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6354f81d-24a6-4ec6-ad3d-d83774ad597b",
   "metadata": {},
   "source": [
    "__Apply these functions into desired data slices.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5c8a614e-a56e-430f-a1af-59417e2cf70b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# All feedbacks\n",
    "# create_folium(data3, filenm = \"cluster_feedback_all.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a4f67c-061c-46cb-ba48-9dd12e386a2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1 map + 1 wordcloud for each category.\n",
    "# By agency in charge.\n",
    "categories = list(data3[\"AGENCY\"].unique())\n",
    "filenms_html = [\"./categories/\" + agency + \".html\" for agency in categories]\n",
    "filenms_png = [\"./categories/\" + agency + \".png\" for agency in categories]\n",
    "\n",
    "for idx, cat in enumerate(categories):\n",
    "    df = data3.loc[data3[\"AGENCY\"] == cat, :]\n",
    "    create_folium(df, filenm = filenms_html[idx])\n",
    "    \n",
    "    text = \" \".join(df[\"DESCRIPTION\"])\n",
    "    cleaned = text_mine(text)    \n",
    "    create_save_cloud(cleaned, max_words = 150, filenm = filenms_png[idx])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa2b9eb-c11b-412c-8888-47798f5ffa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By time of day\n",
    "start_time = [\"00:00:00\", \"08:00:00\", \"19:00:00\"]\n",
    "end_time = [\"07:59:59\", \"18:59:59\", \"23:59:59\"]\n",
    "times = [\"12mn_8am\", \"8am_7pm\", \"7pm_12mn\"]\n",
    "filenms_html = [\"./time/\" + time_day + \".html\" for time_day in times]\n",
    "filenms_png = [\"./time/\" + time_day + \".png\" for time_day in times]\n",
    "\n",
    "for idx, cat in enumerate(times):\n",
    "    \n",
    "    df = data3.set_index(\"DATETIME\").between_time(start_time[0], end_time[0])\n",
    "    create_folium(df, filenm = filenms_html[idx])\n",
    "    \n",
    "    text = \" \".join(df[\"DESCRIPTION\"])\n",
    "    cleaned = text_mine(text)    \n",
    "    create_save_cloud(cleaned, max_words = 150, filenm = filenms_png[idx])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd78fa3c-77f3-4816-a62d-f4d7ba5a70b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By first and last day (1st May and 20th May)\n",
    "start_day = [\"2020-05-01 00:00:00\", \"2020-05-19 00:00:00\"]\n",
    "end_day = [\"2020-05-01 23:59:59\", \"2020-05-19 23:59:59\"]\n",
    "dates = [\"May-01\", \"May-19\"]\n",
    "\n",
    "filenms_html = [\"./day/\" + day + \".html\" for day in dates]\n",
    "filenms_png = [\"./day/\" + day + \".png\" for day in dates]\n",
    "\n",
    "for idx, cat in enumerate(dates):\n",
    "    df = data3.loc[data3['DATETIME'].between(start_day[0], end_day[0]), :]\n",
    "    create_folium(df, filenm = filenms_html[idx])\n",
    "    \n",
    "    text = \" \".join(df[\"DESCRIPTION\"])\n",
    "    cleaned = text_mine(text)    \n",
    "    create_save_cloud(cleaned, max_words = 200, filenm = filenms_png[idx])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41465096-06df-4fc2-8d69-beec804f662d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### *Power users who show strong support in reporting potential residents that flout safe distancing measures*  \n",
    "\n",
    "To drill into 'top 10' users of the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5512727f-6ed7-403b-bed8-98ba0e910487",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3953    107\n",
       "1732     75\n",
       "447      36\n",
       "3126     31\n",
       "3346     30\n",
       "5868     30\n",
       "6297     26\n",
       "3629     24\n",
       "1560     23\n",
       "2522     23\n",
       "Name: NEWCUST_ID, dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3[\"NEWCUST_ID\"].value_counts().nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6aa27054-1953-44ee-a3df-d79afc1b3fbf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3953, 1732, 447, 3126, 3346, 5868, 6297, 3629, 1560, 2522]\n"
     ]
    }
   ],
   "source": [
    "top10 = list(data3[\"NEWCUST_ID\"].value_counts().nlargest(10).index)\n",
    "print(top10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "db3c753a-2796-4ad1-85f7-fd2fcfb599b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User (based on hashed ID): 3953. Unique postal codes reported = ['752510']\n",
      "User (based on hashed ID): 1732. Unique postal codes reported = ['389379', '398721', '389380']\n",
      "User (based on hashed ID): 447. Unique postal codes reported = ['150091', '320082', '320092', '320087', '50335', '89168', '328827', '50005', '50004', '320090', '89167', '80334', '151115', '50336']\n",
      "User (based on hashed ID): 3126. Unique postal codes reported = ['121416', '120418', '121420']\n",
      "User (based on hashed ID): 3346. Unique postal codes reported = ['18955', 'NIL', '39805', '18972', '18940', nan, '307591', '18953']\n",
      "User (based on hashed ID): 5868. Unique postal codes reported = [nan, '679041']\n",
      "User (based on hashed ID): 6297. Unique postal codes reported = [nan, '576462', 'NIL']\n",
      "User (based on hashed ID): 3629. Unique postal codes reported = ['398722', '389379', '398721', '398730', nan]\n",
      "User (based on hashed ID): 1560. Unique postal codes reported = ['NIL', '460420']\n",
      "User (based on hashed ID): 2522. Unique postal codes reported = ['169208', '90017', '109708', '90014']\n"
     ]
    }
   ],
   "source": [
    "# Let's check on each one. First we check based on postal code to see if they are all reporting the same place or not.\n",
    "for top in top10:\n",
    "    postal = list(data3.loc[data3[\"NEWCUST_ID\"] == top, \"POSTAL_CODE\"].unique())\n",
    "    print(\"User (based on hashed ID): {}. Unique postal codes reported = {}\".format(str(top), str(postal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15a37e9-0cf2-4d4d-b31f-39ab92a67df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power users\n",
    "# By agency in charge.\n",
    "filenms_html = [\"./power_users/\" + str(top) + \".html\" for top in top10]\n",
    "filenms_png = [\"./power_users/\" + str(top) + \".png\" for top in top10]\n",
    "filenms_txt = [\"./power_users/\" + str(top) + \".txt\" for top in top10]\n",
    "\n",
    "for idx, top in enumerate(top10):\n",
    "    df = data3.loc[data3[\"NEWCUST_ID\"] == top, :]\n",
    "    create_folium(df, filenm = filenms_html[idx])\n",
    "    \n",
    "    text = \" \".join(df[\"DESCRIPTION\"])\n",
    "    cleaned = text_mine(text)    \n",
    "    create_save_cloud(cleaned, max_words = 150, filenm = filenms_png[idx])\n",
    "    \n",
    "    with open(filenms_txt[idx], \"w\", encoding=\"utf-8\") as file:\n",
    "        file.writelines(\"\\n\".join(df[\"DESCRIPTION\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
